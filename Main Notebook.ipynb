{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQcN0jkpkc_b"
   },
   "source": [
    "#CSC8635: Machine Learning with Project -\n",
    "#Extended Technical Project\n",
    "###Submitted By- Parth Sagar\n",
    "###210431117\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaDC4Q_OVDEV"
   },
   "source": [
    "# Introduction:\n",
    "As per the World Health Organisation, each year approximately 1.3 million people lose their lives because of road accidents.[2] Along with drunk driving and unconsciousness, the major reason that seems to be contributing is distracted driving.\n",
    "Distracted driver detection is a major contributor towards road accidents these days and accounting as public health threat these days.[1] For solving this problem, a video extracted images of distracted drivers’ dataset is used which is easily accessible on Kaggle. A model for real-time detection for distracted drivers and notifying them is required. Moreover, if this is combined with automated driving which is currently pursued by TESLA, will make this world a safer place to drive, and the number of accidents will reduce. It is quite well accepted that computer vision majorly gives the optimum results for image classification.  Hence for this distracted driver detection, CNN was applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3UsxsNWVJTq"
   },
   "source": [
    "# Literature Review:\n",
    "In previous years Distracted driver detection has been a topic that has gathered the attention of many scientists. The work started in the early 90s when the mobile phone started to come into the scenario. Since then, usage of mobile while driving is one of the major contributors to the causes of distracted drivers [1]. With the advancement of technology, this started to increase as the users are increasing and dependence on smartphones is increasing day by day.¬¬ Many researchers have worked on this by making their datasets and applying models to them. Deep neural networks and convolution neural networks (CNN) have been used in the majority of the works till now. Some recent works include the model of ensemble learning for the prediction using video feed this was achieved by using CNN and combining it with Long Short-Term Memory which is part of RNN. This used sensor data along with an image to achieve ensemble learning [2]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atX_RV_AVYNu"
   },
   "source": [
    "Aggregation of CNN and Histogram of Gradients (HOG) has also been used for classification as this uses CNN for image processing and HOG is used for detection of human beings as it collects small gradients, processes their connections, and then normalize to use them for prediction of the regions [3]. Most of these research’s use pre-trained models for processing the images and using them for predictions mostly used are VGG 16, AlexNet, ResNet-50, and Inception V3. These models are used to provide pre-trained weights and their outputs are then flattened to add to certain layers than at the end mapped to categories for output. The number of intermediate layers depends on the researcher. The pretrained model is then combined with Human activity recognition models to detect the features of the driver using K-means Clustering [4]. As the data of sensors and the video, the dataset is not publicly available, and the computational resources are restricted in this project usage of the image is done that too with the pixel size of 160X160. If better computational resources are present, it is recommended to use 256X256 size images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmmqHg19QrVf"
   },
   "source": [
    "Installing json file and unzipping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIHIQlrCF1O9",
    "outputId": "9e40215a-2b71-4a82-9451-51c39aa0672b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█████▋                          | 10 kB 42.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 20 kB 23.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 30 kB 18.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 40 kB 16.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 51 kB 9.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=1325302e0e07467dc81dc5fc2607661401e2b4a84d09ac3ea68faf5362188e8e\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n",
      "Downloading state-farm-distracted-driver-detection.zip to /content\n",
      "100% 3.99G/4.00G [00:33<00:00, 171MB/s]\n",
      "100% 4.00G/4.00G [00:33<00:00, 130MB/s]\n"
     ]
    }
   ],
   "source": [
    "! pip install -q kaggle\n",
    "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle competitions download -c state-farm-distracted-driver-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeC8AZveGBWC"
   },
   "outputs": [],
   "source": [
    "! mkdir driverData\n",
    "! unzip driverData.zip -d driverData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOpCC9MY_UBr"
   },
   "source": [
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR3SwI5_7gRJ",
    "outputId": "68c48791-0bbe-478f-b11b-51585798cf9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasql\n",
      "  Downloading pandasql-0.7.3.tar.gz (26 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.1.5)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.4.29)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.15.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.7.0)\n",
      "Building wheels for collected packages: pandasql\n",
      "  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26784 sha256=4e9eb7cbae91349f60fd582aa74fdb99e81b6f121c7d8303872fab04d7e3b92c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/4b/ec/41f4e116c8053c3654e2c2a47c62b4fca34cc67ef7b55deb7f\n",
      "Successfully built pandasql\n",
      "Installing collected packages: pandasql\n",
      "Successfully installed pandasql-0.7.3\n"
     ]
    }
   ],
   "source": [
    "pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12My0arRGDXS"
   },
   "outputs": [],
   "source": [
    "#loading the libraries\n",
    "#for the usage of array:\n",
    "import numpy as np \n",
    "#for impementing dataset and operations:\n",
    "import pandasql as ps\n",
    "import pandas as pd\n",
    "#for traversing in the dataset:\n",
    "import os\n",
    "#to convert data in pickle files for reproducibility:\n",
    "import pickle\n",
    "#for applying computer vision on images:\n",
    "import cv2\n",
    "#to use image pre processing:\n",
    "from keras.preprocessing import image\n",
    "#to apply models\n",
    "import tensorflow as tf\n",
    "#to plot the charts\n",
    "import matplotlib.pyplot as plt\n",
    "#for applying machine models and layers\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu1147tEVdey"
   },
   "source": [
    "# Data pre-processing:\n",
    "Dataset consists of 22426 images divided into 10 categories namely:\n",
    "1.\tSafe driving \n",
    "2.\tTalking on the phone right\n",
    "3.\tTalking on the phone left\n",
    "4.\tLooking back \n",
    "5.\tTalking to the passenger \n",
    "6.\tDrinking \n",
    "7.\tTexting right\n",
    "8.\tTexting left\n",
    "9.\tHair and makeup \n",
    "10.\tOperating the radio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miabhOo8VpxB"
   },
   "source": [
    "\n",
    "The major challenge with this dataset is that it has quite similar images that confuse the model to easily determine the difference. Therefore, image augmentation is quite necessary for this dataset. \n",
    "Image augmentation types:\n",
    "1. Horizontal flip: flips random images horizontally from the dataset.\n",
    "2. Vertical flip: flips random images vertically from the dataset\n",
    "3. Rotation range: randomly rotates images inside the dataset so our model can be robust.\n",
    "4. Validation split: 0.3 that is 30% data split.\n",
    "5. Fill mode=’nearest’:\n",
    "6. Zoom range: randomly zooming images by 20%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQTpqe0rV4KC"
   },
   "source": [
    "In this project, these augmentations have been tried at different stages with different values and the values mentioned above are giving the best results. These augmentations have been applied inside through computer vision library CV2 and image data generator class, varying with the way data was considered.\n",
    "There were 3 approaches considered on the dataset, those are providing the data through google drive and doing a train test split through a Keras function, taking the data from the drive and partitioning it based on people out of the 26 being considered for the study and the last one is partitioning through image data generator class. First, the Train test split was done by taking 30% as validation data and 70% as training data. Second, the Train test split based on people: as 26 participants have accounted for 22424 images, we randomly chose 6 people and added them to the test set so that training data and test data could be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMtW_7WkV706"
   },
   "source": [
    "Input shapes: (32,32,3), (64,64,3), (128,128,3) and (128,128,1). Images were imported on different pixel sizes and accuracy was drastically dropping in lower pixel sizes.  At 128 X 128 RGB to Gray was also considered to see if that enables models to get better accuracy with a lower number of hyperparameters and, hence, saves time and speed of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVZMpYdZV_eL"
   },
   "source": [
    "Resizing of image: all the images are converted to NumPy array and then resized by 1/255 as the images have to be normalized to apply the image analysis. We can decide the batch size so that we can define how many photos are present in each batch. That is required as batch normalization needs to be applied which we will be discussed later in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqysAoAjGOV6",
    "outputId": "b4caeaa0-3e08-40bd-933a-a8deb7ee940d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22424 images belonging to 10 classes.\n",
      "Found 6722 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image augmentation and Train test split\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   validation_split=0.3,\n",
    "                                   rotation_range=180,\n",
    "                                   #shear_range=0.2,\n",
    "                                   zoom_range=0.5,\n",
    "                                   #horizontal_flip=True,\n",
    "                                   #vertical_flip=True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   #featurewise_center=True,\n",
    "                                   ) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/content/driverData/imgs/train/',\n",
    "    target_size=(160, 160),\n",
    "    batch_size=128,\n",
    "    ) # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/content/driverData/imgs/train/',  # same directory as training data\n",
    "    target_size=(160, 160),\n",
    "    batch_size=128,\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITvaQsrBAuad"
   },
   "outputs": [],
   "source": [
    "#defining seish activation function\n",
    "from keras.backend import sigmoid\n",
    "def swish(x,beta=1):\n",
    "    return(x*sigmoid(beta*x))\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation \n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWWNUSc0WXEy"
   },
   "source": [
    "## Exploratory data analysis:\n",
    "There are certain questions that how the data is distributed and how is it justifiable to consider the data? \n",
    "So, to answer these questions the distribution of data was considered, how many photos of each person are present is taken into account and the uniformity with categories is checked.\n",
    "How many photos of each category are present? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F63WWrWR7fay"
   },
   "outputs": [],
   "source": [
    "# reading the csv\n",
    "df1=pd.read_csv('/content/driverData/driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_479riw_Wecf"
   },
   "outputs": [],
   "source": [
    "# finding the number of images using SQL\n",
    "df2=ps.sqldf('select classname, count(img) as Number_of_images from df1 group by classname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "bxwEB5aCWoWZ",
    "outputId": "e7689d18-361c-4ec5-a50e-79d2a647ede1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'No. of Images')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW6UlEQVR4nO3dfbRddX3n8fdHQFwClSCRYhKNw4R20EHQDFLtqGgHEKcgVl2wfAiKk2KhxdG6ih1HUJbV1vqwtIhGTQlOFbFKjQ4jTRFBHbEExPAkkiJIYoQICCqDFf3OH/t39RTuvftwyTn3xvt+rXXW2fu39z7f37l5+Nz99NupKiRJms7DZrsDkqS5z7CQJPUyLCRJvQwLSVIvw0KS1GvH2e7AKOy55561dOnS2e6GJG1XLr/88h9U1cLJlv1ahsXSpUtZv379bHdDkrYrSW6eapmHoSRJvQwLSVIvw0KS1MuwkCT1MiwkSb1GFhZJliS5KMm1Sa5JcnJrPy3J5iRXttcRA9u8McnGJNcnOWyg/fDWtjHJKaPqsyRpcqO8dPY+4PVVdUWS3YDLk6xry95TVX89uHKS/YBjgCcCjwX+Kcm+bfEZwH8BNgGXJVlbVdeOsO+SpAEjC4uq2gJsadM/SnIdsGiaTY4CzqmqnwLfSbIROKgt21hVNwIkOaeta1hI0piM5ZxFkqXAgcDXW9NJSTYkWZ1kQWtbBNwysNmm1jZV+/1rrEyyPsn6rVu3buNvIEnz28jv4E6yK/Bp4LVVdXeSM4HTgWrv7wJe9VDrVNUqYBXA8uXLH9ITnZae8r8fand63fSO54+8hiRtKyMNiyQ70QXF31XVZwCq6taB5R8GPt9mNwNLBjZf3NqYpl2SNAajvBoqwEeB66rq3QPtew+sdjRwdZteCxyTZOckTwCWAf8MXAYsS/KEJA+nOwm+dlT9liQ90Cj3LJ4BvBy4KsmVre3PgWOTHEB3GOom4A8BquqaJOfSnbi+Dzixqn4OkOQk4AJgB2B1VV0zwn5Lku5nlFdDfQXIJIvOn2abtwFvm6T9/Om2kySNlndwS5J6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqdfIx4aShuF4XNLcZlho3puvQTVfv7dmxrDQL/mfx/j5M9f2wnMWkqRehoUkqZdhIUnq5TmLOcZj2JLmIvcsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsaEkaUy257Hf3LOQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb1GFhZJliS5KMm1Sa5JcnJr3yPJuiQ3tPcFrT1J3pdkY5INSZ4y8Fkr2vo3JFkxqj5LkiY3yj2L+4DXV9V+wMHAiUn2A04BLqyqZcCFbR7gecCy9loJnAlduACnAk8DDgJOnQgYSdJ4jCwsqmpLVV3Rpn8EXAcsAo4C1rTV1gAvaNNHAWdX51Jg9yR7A4cB66rqjqq6E1gHHD6qfkuSHmgsd3AnWQocCHwd2KuqtrRF3wf2atOLgFsGNtvU2qZqv3+NlXR7JDzucY/bdp2XtM1tz3cyz1cjP8GdZFfg08Brq+ruwWVVVUBtizpVtaqqllfV8oULF26Lj5QkNSMNiyQ70QXF31XVZ1rzre3wEu39tta+GVgysPni1jZVuyRpTEZ5NVSAjwLXVdW7BxatBSauaFoBfHag/RXtqqiDgbva4aoLgEOTLGgntg9tbZKkMRnlOYtnAC8HrkpyZWv7c+AdwLlJjgduBl7Slp0PHAFsBO4BXglQVXckOR24rK331qq6Y4T9liTdz8jCoqq+AmSKxc+dZP0CTpzis1YDq7dd7yRJD4Z3cEuSehkWkqRePilP0rziPR4z456FJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF69YZFklyQPa9P7JjkyyU6j75okaa4YZs/iEuARSRYB/wi8HDhrlJ2SJM0tw4RFquoe4IXAB6rqxcATR9stSdJcMlRYJPkd4KXAxMNrdxhdlyRJc80wYfFa4I3AeVV1TZJ/B1w02m5JkuaSHftWqKqLgYuTPLLN3wj8yag7JkmaO4a5Gup3klwLfKvNPznJB0beM0nSnDHMYaj3AocBtwNU1TeBZ46yU5KkuWWom/Kq6pb7Nf18BH2RJM1RvecsgFuSPB2odjPeycB1o+2WJGkuGWbP4gTgRGARsBk4oM1LkuaJYa6G+gHdPRaSpHmqNyySvG+S5ruA9VX12W3fJUnSXDPMYahH0B16uqG99gcWA8cnee8I+yZJmiOGCYv9gUOq6v1V9X7g94DfBo4GDp1qoySrk9yW5OqBttOSbE5yZXsdMbDsjUk2Jrk+yWED7Ye3to1JTpnJl5QkPTTDhMUCYNeB+V2AParq58BPp9nuLODwSdrfU1UHtNf5AEn2A46hG6DwcOADSXZIsgNwBvA8YD/g2LauJGmMhrl09q+AK5N8CQjdDXl/kWQX4J+m2qiqLkmydMh+HAWcU1U/Bb6TZCNwUFu2sQ0xQpJz2rrXDvm5kqRtoHfPoqo+Cjwd+AfgPOB3q+ojVfWTqnrDDGqelGRDO0y1oLUtAgZv/NvU2qZqf4AkK5OsT7J+69atM+iWJGkqwz5W9V5gC3An8O+TzHS4jzOBfehOmG8B3jXDz3mAqlpVVcuravnChQu31cdKkhju0tlX0921vRi4EjgY+BrwnAdbrKpuHfjcDwOfb7ObgSUDqy5ubUzTLkkak2H2LE4G/hNwc1UdAhwI/HAmxZLsPTB7NDBxpdRa4JgkOyd5ArAM+GfgMmBZkickeTjdSfC1M6ktSZq5YU5w31tV9yYhyc5V9a0kv9W3UZJPAM8G9kyyCTgVeHaSA4ACbgL+EKA9VOlcuhPX9wEntqutSHIScAHd0/lWV9U1D/ZLSpIemmHCYlOS3elOcK9Lcidwc99GVXXsJM0fnWb9twFvm6T9fOD8IfopSRqRYcaGOrpNnpbkIuBRwBdG2itJ0pwy1NVQSRYk2R/4Ed3lq08aaa8kSXPKMFdDnQ4cB9wI/KI1FzO4GkqStH0a5pzFS4B9qupfR90ZSdLcNMxhqKuB3UfdEUnS3DXMnsXbgW+00WN/OXBgVR05sl5JkuaUYcJiDfCXwFX86pyFJGkeGSYs7qmqyZ6WJ0maJ4YJiy8neTvdMBuDh6GuGFmvJElzyjBhcWB7P3igzUtnJWkeGeYO7kPG0RFJ0tw1ZVgked10G1bVu7d9dyRJc9F0exa7ja0XkqQ5bcqwqKq3jLMjkqS5a9jHqkqS5jHDQpLUa8qwSHJye3/G+LojSZqLptuzeGV7f/84OiJJmrumuxrquiQ3AI9NsmGgPUBV1f6j7Zokaa6Y7mqoY5P8JnAB4AizkjSPTXsHd1V9H3hykocD+7bm66vqZyPvmSRpzhjmsarPAs4GbqI7BLUkyYqqumTEfZMkzRHDDCT4buDQqroeIMm+wCeAp46yY5KkuWOY+yx2mggKgKr6NrDT6LokSZprhtmzWJ/kI8D/avMvBdaPrkuSpLlmmLB4DXAi8Cdt/svAB0bWI0nSnDPM8yx+SnfewiHJJWmecmwoSVIvw0KS1MuwkCT1mlFYJFm5rTsiSZq7ZrpnkW3aC0nSnDajsKiqD/Wtk2R1ktuSXD3QtkeSdUluaO8LWnuSvC/JxiQbkjxlYJsVbf0bkqyYSX8lSQ9Nb1gkWZzkvCRb23/+n06yeIjPPgs4/H5tpwAXVtUy4MI2D/A8YFl7rQTObLX3AE4FngYcBJw6ETCSpPEZZs/ib4G1wN7AY4HPtbZptYEG77hf81HAmja9BnjBQPvZ1bkU2D3J3sBhwLqquqOq7gTW8cAAkiSN2DBhsbCq/raq7muvs4CFM6y3V1VtadPfB/Zq04uAWwbW29TapmqXJI3RMGFxe5KXJdmhvV4G3P5QC1dVAfVQP2dCkpVJ1idZv3Xr1m31sZIkhguLVwEvodsT2AK8iF89n/vBurUdXqK939baNwNLBtZb3Nqman+AqlpVVcuravnChTPd8ZEkTaY3LKrq5qo6sqoWVtVjquoFVfXdGdZbC0xc0bQC+OxA+yvaVVEHA3e1w1UXAIcmWdBObB/a2iRJYzTlQIJJ3jzNdlVVp0/3wUk+ATwb2DPJJrqrmt4BnJvkeOBmuj0WgPOBI4CNwD20PZequiPJ6cBlbb23VtX9T5pLkkZsulFnfzJJ2y7A8cCjgWnDoqqOnWLRcydZt+iGQZ/sc1YDq6erJUkarSnDoqreNTGdZDfgZLrf+M8B3jXVdpKkXz/TPs+i3RT3Orqn460BntLud5AkzSPTnbN4J/BCYBXwH6vqx2PrlSRpTpnuaqjX092x/Sbge0nubq8fJbl7PN2TJM0F052z8FkXkiTAhx9JkoZgWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqdeshEWSm5JcleTKJOtb2x5J1iW5ob0vaO1J8r4kG5NsSPKU2eizJM1ns7lncUhVHVBVy9v8KcCFVbUMuLDNAzwPWNZeK4Ezx95TSZrn5tJhqKOANW16DfCCgfazq3MpsHuSvWejg5I0X81WWBTwj0kuT7Kyte1VVVva9PeBvdr0IuCWgW03tbZ/I8nKJOuTrN+6deuo+i1J89KOs1T3d6tqc5LHAOuSfGtwYVVVknowH1hVq4BVAMuXL39Q20qSpjcrexZVtbm93wacBxwE3DpxeKm939ZW3wwsGdh8cWuTJI3J2MMiyS5JdpuYBg4FrgbWAivaaiuAz7bptcAr2lVRBwN3DRyukiSNwWwchtoLOC/JRP2PV9UXklwGnJvkeOBm4CVt/fOBI4CNwD3AK8ffZUma38YeFlV1I/DkSdpvB547SXsBJ46ha5KkKcylS2clSXOUYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqdd2ExZJDk9yfZKNSU6Z7f5I0nyyXYRFkh2AM4DnAfsBxybZb3Z7JUnzx3YRFsBBwMaqurGq/hU4BzhqlvskSfNGqmq2+9AryYuAw6vq1W3+5cDTquqkgXVWAivb7G8B14+xi3sCPxhjPWtb29rzp/44az++qhZOtmDHMXVg5KpqFbBqNmonWV9Vy61tbWv/+tWe7fqz/d0nbC+HoTYDSwbmF7c2SdIYbC9hcRmwLMkTkjwcOAZYO8t9kqR5Y7s4DFVV9yU5CbgA2AFYXVXXzHK3Bs3K4S9rW9va86L+bH93YDs5wS1Jml3by2EoSdIsMiwkSb0Mi20gyc5JPtmGIvl6kqVjrP3MJFckua/djzI2SV6X5NokG5JcmOTxY6x9QpKrklyZ5CuzcUd/kj9IUknGdlljkuOSbG3f+8okrx5X7Vb/Je3P/JokHx9j3fcMfOdvJ/nhGGs/LslFSb7R/q4fMcbaj2//tjYk+VKSxeOq/QBV5eshvoA/Aj7Ypo8BPjnG2kuB/YGzgReN+XsfAjyyTb9mzN/7NwamjwS+MObvvhtwCXApsHyMdY8D/mac33Wg9jLgG8CCNv+YWerHH9Nd5DKuequA17Tp/YCbxlj7U8CKNv0c4GOz8TOvKvcsZiLJK1rSfzPJx+iGHlnTFv898NwkGUftqrqpqjYAvxhFvZ7aF1XVPW3xpXT3v4yr9t0Di3cBRnalxiR/3gCnA38J3DuqutPUHotJav834IyquhOgqm4bY+1BxwKfGGPtAn6jLX4U8L0x1t4P+GJbfBGzOczRbKXU9voCngh8G9izze8BXA0sHljnXyaWj7r2wLKzGOGexXS12/zfAG8aZ23gxPazvgVYNsY/76cAn27zX2JEexZT1D4O2AJsoPvFZMkYa/8D8FfAV+l+OTh83H/XgMe377/DGL/33sBVwCbgTuCpY6z9ceDkNv9CuuB69Cjq973cs3jwngN8qqp+AFBVd8z32kleBiwH3jnO2lV1RlXtA/wZ8KZx1AZ+CLwbeP2I6k1Zu33vzwFLq2p/YB2/2qMdR+0d6Q5FPZvut/sPJ9l9TLUnHAP8fVX9fAR1p6p9LHBWVS0GjgA+lmQU/3dOVvtPgWcl+QbwLLqRK0b13adlWGwbvxyOJMmOdLuqt89qj8Ykye8B/wM4sqp+OkvdOAd4wZhq7QY8CfhSkpuAg4G14zrJXVW3D/ycPwI8dRx1m03A2qr6WVV9h+634GVjrA9dWIzsENQUjgfOBaiqrwGPoBvcb+Sq6ntV9cKqOpDu3xlVNbaT+4MMiwfvi8CLkzwaIMkedEOPrGjLXwR8sdp+4xhqj8sDaic5EPgQXVCM7Pj1FLUH/5N6PnDDOGrTHf7Ys6qWVtVSusMxR1bV+lHXbt9774HlRwLXjaDupLXpDkM9u83vCewL3Dim2iT5bWAB8LUR1Jyu9neB57b5/0AXFlvHUTvJngN7MW8EVo+g7lC2i+E+5pKquibJ24CLk/yc7uqQE+h2TTcCd9D99jOW2knOAM6j+0f0+0neUlVPHEdtuhPauwKfaufzv1tVR46p9l1tr+ZndMeRV0z3Gdu49nGjqDVk7S1JjgTuo/u7NpK+TFH7lcChSa6lOxTyhqra5nvQ0/zMjwHOGdEvYtPVfj3dIbf/TnfO4LhR9GGK2p8H3p6k6K6+O3Fb1x2Ww31Iknp5GEqS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJCaJL+Z5Jwk/5Lk8iTnJ9l3inV3T/JHY+rXCUleMY5a0lS8dFYC2sCP/xdYU1UfbG1Pphvd9suTrL8U+HxVPWnE/dqxqu4bZQ1pGO5ZSJ1DgJ9NBAVAVX2T7sbHC9M9M+SqJBOjfr4D2Cfd8xXeCZDkDUkua6OGvmXic5L8zyTXp3vuxieS/GlrPyDJpW3985IsaO1fSvLeJOuBk5OcNrDNPkm+0PZ8vtzuaibJi5Nc3UYrvWQMPy/NM97BLXWeBFw+Sfu9wNFVdXcb4uLSJGuBU4AnVdUBAEkOpRsn6SAgdONFPRP4f8AfAE8GdgKuGKhzNvDHVXVxkrcCpwKvbcseXlXL22efNtCfVcAJVXVDkqcBH6AbgO7NwGFVtTmjGdxP85xhIU0vwF+0//h/ASwC9ppkvUPb6xttfle68NgN+GxV3Qvcm+RzAEkeBexeVRe39dfQPehmwicf0JFkV+Dp/Gp4FYCd2/tXgbOSnAt8ZgbfU5qWYSF1rqEbBPL+XgospHuGwc/SjTT7iEnWC/D2qvrQv2lMXjvJusP4ySRtDwN+OLE3M6iqTmh7Gs8HLk/y1FGM26T5y3MWUueLwM5JVk40JNmf7mE7t7WgOKTNA/yIbq9hwgXAq9pv/yRZlOQxdL/x/36SR7Rl/xWgqu4C7kzyn9v2LwcuZhrVPR3wO0le3GqknYQnyT5V9fWqejPdiKhLZvyTkCbhnoUEVFUlORp4b5I/oztXcRNwGvC+JFcB64FvtfVvT/LVJFcD/6eq3pBu+OqvtUNEPwZeVlWXtXMcG4Bb6Z64dlcruwL4YJJH0g31/cohuvpS4Mwkb6I7B3IO8E3gnemGbQ9wYWuTthkvnZVGLMmuVfXjFgqXACur6orZ7pf0YLhnIY3eqiT70Z3rWGNQaHvknoUkqZcnuCVJvQwLSVIvw0KS1MuwkCT1MiwkSb3+P8M64JaqLmH5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making a bar plot f the data\n",
    "plt.bar(df2.classname, df2.Number_of_images)\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"No. of Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcLUjgVkXZzK"
   },
   "source": [
    "As the data suggests there is an almost equal number of images in each category. That suggests the model can be easily trained on these images as no particular category would dominate others.\n",
    "\n",
    "How many pictures of each driver are present? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "5pEaQ6KKXkm8",
    "outputId": "f022325b-175e-478f-d614-0a9d8b786dd1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2f51937f-64c9-49a8-b591-b11cf74318c5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Number_of_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p014</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p015</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p016</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p021</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p022</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p024</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p026</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p035</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>p039</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p041</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p042</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>p045</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p047</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p049</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p050</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p051</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>p052</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>p056</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p061</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>p064</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>p066</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>p072</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>p075</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>p081</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f51937f-64c9-49a8-b591-b11cf74318c5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2f51937f-64c9-49a8-b591-b11cf74318c5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2f51937f-64c9-49a8-b591-b11cf74318c5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   subject  Number_of_images\n",
       "0     p002               725\n",
       "1     p012               823\n",
       "2     p014               876\n",
       "3     p015               875\n",
       "4     p016              1078\n",
       "5     p021              1237\n",
       "6     p022              1233\n",
       "7     p024              1226\n",
       "8     p026              1196\n",
       "9     p035               848\n",
       "10    p039               651\n",
       "11    p041               605\n",
       "12    p042               591\n",
       "13    p045               724\n",
       "14    p047               835\n",
       "15    p049              1011\n",
       "16    p050               790\n",
       "17    p051               920\n",
       "18    p052               740\n",
       "19    p056               794\n",
       "20    p061               809\n",
       "21    p064               820\n",
       "22    p066              1034\n",
       "23    p072               346\n",
       "24    p075               814\n",
       "25    p081               823"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=ps.sqldf('select subject, count(img) as Number_of_images from df1 group by subject')\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwMIdF_kWn2i"
   },
   "source": [
    "This was needed to be checked as if one particular driver dominates the whole dataset because if that isthe case there are chances that the model would start classifying based on that driver’s features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2fBfJEEYVx6"
   },
   "source": [
    "## Input and Output Layers components:\n",
    "1.\tConv 2D: convolution layer of 2 dimensions is run over an image using a spatial matrix which uses filters which are values of the 2 dimensions, strides these are the number of pixels to skip by the filter, kernel size is the filter size for the image.\n",
    "2.\tMax pooling:  CNN works by extracting the features from the image. For example, in the texting right category, we have a mobile phone in the right hand as one of the features. Now as the spatial matrix traverse over the image to learn it might learn some features as it is. So the model will not predict if it is rotated or present in a different scenario. Down sampling that is reducing the resolution is required. Hence max pooling is one of the types of down sampling which considers the matrix of a certain size chooses the maximum value out of those pixels and replaces it instead.\n",
    "3.\tDense: dense layer is a fully connected layer to its preceding layer and it takes all the neurons information from the previous layer and adds bias to the value. Then the value is passed to the activation function which determines the value of this node.\n",
    "4.\tDropout: dropout layer works with setting the neurons input values with 0. It doesn’t let the model overfit and replaces the values with 0 and change the other values with 1/(1-rate) such that the sum of all inputs is unchanged.\n",
    "5.\tBatch normalization: batch normalization works to maintain mean output as 0 and works to maintain standard deviation of output at 1. This is a transformation that works in 2 different ways when applied to training or testing samples for training it needs to be applied during fit function and for testing it is applied at evaluation.\n",
    "6.\tFilter size: the filter size works to extract the features as the filters move over an image it returns the value by multiplying an n-dimensional matrix which only has the values over the diagonal and those values are 1. It extracts those features and returns the value. The filter size that is the value of n depends on the training data and type of features.\n",
    "7.\tInput shape: the input shape depends on the images’ pixel size, the features needed for classification, and the available computational resources. \n",
    "8.\tActivation functions:\n",
    "    1.\tTanh: tanh function is straight from the trigonometry returns the value of (exp(x)-exp(-x)) /(exp(x)+exp(-x))\n",
    "    8.\tSigmoid: Sigmoid function returns the values between 0 and 1 it uses a function that considers the equation of 1/1+exp(-x). So, it returns the value nearby zero if the value of x is small and near to 1 if the value of X is big.\n",
    "    8.\tReLu: Rectified Linear unit is a function that returns the value of the maximum of 2 values in which the first is 0 and the second is x (the input vector). This returns all the positive values.\n",
    "    8.\tSwish: this is a fairly new activation function; it has not been included in the official Keras documentation till now. It is an extension of the sigmoid function that is it returns the value of x*sigmoid(x).\n",
    "    8.\tSoftMax: The SoftMax function works on the idea of probability distribution and returns the output vector whose values are in the range of 0 and 1. Also, the sum of these output vectors is 1. This is majorly used in the last layer of the model for classification purposes.\n",
    "9.\tOptimizers used:\n",
    "    1.\t SGD: stochastic gradient descent is an originally robust model that shapes most optimizers today. Sgd learning power in the model is bit less than and takes more than 40 epochs to learn to optimum value. Its learning rate is kept at 0.01. If kept at lower value model might be underfit.\n",
    "    2.\t ADAM: Adam is the method that is based on the adaptive estimation of stochastic gradient descent. It is the most used optimizer. Its learning rate is 0.001. As Adam is very powerfull it tends to overfit on the model with validation accuracy being low. So The learning rate has been kept low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXbUBQehY9rl"
   },
   "source": [
    "# Methodology:\n",
    "The model chosen here is sequential, the first convolution layer has 64 nodes and a 16X16 filter for adequate feature attraction. The activation function used for the first layer is swish. Max pooling filter of 2X2 used here for a down sampling. After this, a dropout layer is introduced with a 0.2 value that is 20 percent of the nodes are replaced with 0, and the rest are multiplied by 1/0.8. After that, nodes are increased to 128 so that the features can be further divided and processed again the same filter of 16X16 is passed to analyze and activation functions are kept the same. Here max-pooling of 4X4 is added with strides value as 2 which means this will skip 1 in between for down sampling. Again, a dropout of 20% is used. After this, the output is flattened and mapped with a hidden layer of 1024 units where activation function ReLu is used. Then the next hidden layer concise the size to converge the learning of the nodes and drive towards the conclusion hence has 256 nodes with a dropout of 0.2. The next layer has 128 nodes with activation function swish. Then dropout with 0.2 is introduced here which finally maps with the output layer which returns a predicted value which is one hot encoded and this is then deciphered to know the predicted category and check the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPWuKVEJSCsK"
   },
   "source": [
    "CNN 6 Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L95-YVlHGaVP",
    "outputId": "f719dfc8-86d7-40d0-ebbe-0d20816b84a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 160, 160, 64)      49216     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 80, 80, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 80, 80, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 80, 80, 128)       2097280   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 39, 39, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 39, 39, 128)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 194688)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              199361536 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201,804,618\n",
      "Trainable params: 201,804,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "176/176 [==============================] - 280s 1s/step - loss: 2.3007 - accuracy: 0.1052 - val_loss: 2.2994 - val_accuracy: 0.1206\n",
      "Epoch 2/70\n",
      "176/176 [==============================] - 248s 1s/step - loss: 2.2989 - accuracy: 0.1135 - val_loss: 2.2977 - val_accuracy: 0.1134\n",
      "Epoch 3/70\n",
      "176/176 [==============================] - 249s 1s/step - loss: 2.2976 - accuracy: 0.1156 - val_loss: 2.2960 - val_accuracy: 0.1428\n",
      "Epoch 4/70\n",
      "176/176 [==============================] - 248s 1s/step - loss: 2.2949 - accuracy: 0.1192 - val_loss: 2.2932 - val_accuracy: 0.1397\n",
      "Epoch 5/70\n",
      "176/176 [==============================] - 248s 1s/step - loss: 2.2914 - accuracy: 0.1303 - val_loss: 2.2888 - val_accuracy: 0.1532\n",
      "Epoch 6/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 2.2825 - accuracy: 0.1415 - val_loss: 2.2743 - val_accuracy: 0.1691\n",
      "Epoch 7/70\n",
      "176/176 [==============================] - 248s 1s/step - loss: 2.2647 - accuracy: 0.1607 - val_loss: 2.2471 - val_accuracy: 0.1747\n",
      "Epoch 8/70\n",
      "176/176 [==============================] - 248s 1s/step - loss: 2.2290 - accuracy: 0.1826 - val_loss: 2.1994 - val_accuracy: 0.2066\n",
      "Epoch 9/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 2.1794 - accuracy: 0.2072 - val_loss: 2.1332 - val_accuracy: 0.2478\n",
      "Epoch 10/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 2.1080 - accuracy: 0.2372 - val_loss: 2.0463 - val_accuracy: 0.2746\n",
      "Epoch 11/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 2.0014 - accuracy: 0.2789 - val_loss: 1.9641 - val_accuracy: 0.3091\n",
      "Epoch 12/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 1.8547 - accuracy: 0.3360 - val_loss: 1.7295 - val_accuracy: 0.3948\n",
      "Epoch 13/70\n",
      "176/176 [==============================] - 245s 1s/step - loss: 1.7148 - accuracy: 0.3812 - val_loss: 1.5820 - val_accuracy: 0.4466\n",
      "Epoch 14/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 1.5727 - accuracy: 0.4315 - val_loss: 1.4070 - val_accuracy: 0.5077\n",
      "Epoch 15/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 1.4564 - accuracy: 0.4713 - val_loss: 1.2671 - val_accuracy: 0.5635\n",
      "Epoch 16/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 1.3349 - accuracy: 0.5181 - val_loss: 1.1800 - val_accuracy: 0.5774\n",
      "Epoch 17/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 1.2452 - accuracy: 0.5525 - val_loss: 1.0811 - val_accuracy: 0.6244\n",
      "Epoch 18/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 1.1666 - accuracy: 0.5827 - val_loss: 1.0052 - val_accuracy: 0.6498\n",
      "Epoch 19/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 1.0798 - accuracy: 0.6158 - val_loss: 0.9121 - val_accuracy: 0.6871\n",
      "Epoch 20/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 1.0207 - accuracy: 0.6376 - val_loss: 0.8578 - val_accuracy: 0.7109\n",
      "Epoch 21/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.9543 - accuracy: 0.6605 - val_loss: 0.7859 - val_accuracy: 0.7395\n",
      "Epoch 22/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.9058 - accuracy: 0.6816 - val_loss: 0.7383 - val_accuracy: 0.7599\n",
      "Epoch 23/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.8462 - accuracy: 0.7038 - val_loss: 0.6900 - val_accuracy: 0.7650\n",
      "Epoch 24/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.7890 - accuracy: 0.7279 - val_loss: 0.6379 - val_accuracy: 0.7917\n",
      "Epoch 25/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.7621 - accuracy: 0.7376 - val_loss: 0.5977 - val_accuracy: 0.8091\n",
      "Epoch 26/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.7285 - accuracy: 0.7471 - val_loss: 0.5579 - val_accuracy: 0.8160\n",
      "Epoch 27/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.6897 - accuracy: 0.7649 - val_loss: 0.5504 - val_accuracy: 0.8124\n",
      "Epoch 28/70\n",
      "176/176 [==============================] - 245s 1s/step - loss: 0.6593 - accuracy: 0.7756 - val_loss: 0.4896 - val_accuracy: 0.8441\n",
      "Epoch 29/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.6257 - accuracy: 0.7836 - val_loss: 0.4900 - val_accuracy: 0.8384\n",
      "Epoch 30/70\n",
      "176/176 [==============================] - 245s 1s/step - loss: 0.5931 - accuracy: 0.7997 - val_loss: 0.4725 - val_accuracy: 0.8515\n",
      "Epoch 31/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.5737 - accuracy: 0.8057 - val_loss: 0.4273 - val_accuracy: 0.8677\n",
      "Epoch 32/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.5511 - accuracy: 0.8139 - val_loss: 0.4825 - val_accuracy: 0.8373\n",
      "Epoch 33/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.5365 - accuracy: 0.8177 - val_loss: 0.3930 - val_accuracy: 0.8735\n",
      "Epoch 34/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.4985 - accuracy: 0.8324 - val_loss: 0.3607 - val_accuracy: 0.8840\n",
      "Epoch 35/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.4933 - accuracy: 0.8330 - val_loss: 0.3585 - val_accuracy: 0.8863\n",
      "Epoch 36/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.4737 - accuracy: 0.8446 - val_loss: 0.3681 - val_accuracy: 0.8801\n",
      "Epoch 37/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.4611 - accuracy: 0.8501 - val_loss: 0.3316 - val_accuracy: 0.8990\n",
      "Epoch 38/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.4447 - accuracy: 0.8516 - val_loss: 0.3182 - val_accuracy: 0.9017\n",
      "Epoch 39/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.4250 - accuracy: 0.8601 - val_loss: 0.3081 - val_accuracy: 0.9035\n",
      "Epoch 40/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.4209 - accuracy: 0.8594 - val_loss: 0.3021 - val_accuracy: 0.9008\n",
      "Epoch 41/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.4039 - accuracy: 0.8676 - val_loss: 0.3018 - val_accuracy: 0.9076\n",
      "Epoch 42/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3884 - accuracy: 0.8705 - val_loss: 0.2889 - val_accuracy: 0.9101\n",
      "Epoch 43/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.3777 - accuracy: 0.8754 - val_loss: 0.2760 - val_accuracy: 0.9133\n",
      "Epoch 44/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3682 - accuracy: 0.8776 - val_loss: 0.2745 - val_accuracy: 0.9109\n",
      "Epoch 45/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3530 - accuracy: 0.8837 - val_loss: 0.2561 - val_accuracy: 0.9179\n",
      "Epoch 46/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.3491 - accuracy: 0.8838 - val_loss: 0.2462 - val_accuracy: 0.9255\n",
      "Epoch 47/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.3345 - accuracy: 0.8927 - val_loss: 0.2281 - val_accuracy: 0.9281\n",
      "Epoch 48/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3319 - accuracy: 0.8910 - val_loss: 0.2303 - val_accuracy: 0.9283\n",
      "Epoch 49/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3220 - accuracy: 0.8949 - val_loss: 0.2291 - val_accuracy: 0.9296\n",
      "Epoch 50/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3107 - accuracy: 0.9009 - val_loss: 0.2251 - val_accuracy: 0.9293\n",
      "Epoch 51/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.3004 - accuracy: 0.9036 - val_loss: 0.2241 - val_accuracy: 0.9274\n",
      "Epoch 52/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.3044 - accuracy: 0.9018 - val_loss: 0.2016 - val_accuracy: 0.9354\n",
      "Epoch 53/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2898 - accuracy: 0.9069 - val_loss: 0.2028 - val_accuracy: 0.9365\n",
      "Epoch 54/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2811 - accuracy: 0.9088 - val_loss: 0.1895 - val_accuracy: 0.9424\n",
      "Epoch 55/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2745 - accuracy: 0.9132 - val_loss: 0.1904 - val_accuracy: 0.9402\n",
      "Epoch 56/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.2756 - accuracy: 0.9125 - val_loss: 0.1939 - val_accuracy: 0.9406\n",
      "Epoch 57/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.2637 - accuracy: 0.9145 - val_loss: 0.1857 - val_accuracy: 0.9429\n",
      "Epoch 58/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2595 - accuracy: 0.9172 - val_loss: 0.2216 - val_accuracy: 0.9293\n",
      "Epoch 59/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2526 - accuracy: 0.9203 - val_loss: 0.1733 - val_accuracy: 0.9458\n",
      "Epoch 60/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.2518 - accuracy: 0.9185 - val_loss: 0.1612 - val_accuracy: 0.9487\n",
      "Epoch 61/70\n",
      "176/176 [==============================] - 247s 1s/step - loss: 0.2421 - accuracy: 0.9241 - val_loss: 0.1608 - val_accuracy: 0.9519\n",
      "Epoch 62/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2352 - accuracy: 0.9249 - val_loss: 0.1543 - val_accuracy: 0.9514\n",
      "Epoch 63/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2329 - accuracy: 0.9261 - val_loss: 0.1514 - val_accuracy: 0.9509\n",
      "Epoch 64/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2254 - accuracy: 0.9266 - val_loss: 0.1626 - val_accuracy: 0.9484\n",
      "Epoch 65/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2274 - accuracy: 0.9281 - val_loss: 0.1720 - val_accuracy: 0.9441\n",
      "Epoch 66/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2183 - accuracy: 0.9304 - val_loss: 0.1367 - val_accuracy: 0.9591\n",
      "Epoch 67/70\n",
      "176/176 [==============================] - 244s 1s/step - loss: 0.2135 - accuracy: 0.9307 - val_loss: 0.1449 - val_accuracy: 0.9545\n",
      "Epoch 68/70\n",
      "176/176 [==============================] - 244s 1s/step - loss: 0.2146 - accuracy: 0.9307 - val_loss: 0.1315 - val_accuracy: 0.9594\n",
      "Epoch 69/70\n",
      "176/176 [==============================] - 246s 1s/step - loss: 0.2030 - accuracy: 0.9359 - val_loss: 0.1330 - val_accuracy: 0.9591\n",
      "Epoch 70/70\n",
      "176/176 [==============================] - 245s 1s/step - loss: 0.2055 - accuracy: 0.9346 - val_loss: 0.1423 - val_accuracy: 0.9555\n",
      "Model Training Complete...\n"
     ]
    }
   ],
   "source": [
    "model6=Sequential()\n",
    "#input layer\n",
    "model6.add(Conv2D(64, (16,16), activation='swish', padding='same', input_shape=(160,160,3)))\n",
    "model6.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "model6.add(Dropout(0.2))\n",
    "#second layer\n",
    "model6.add(Conv2D(128, (16,16), activation='swish', padding='same'))\n",
    "model6.add(MaxPooling2D(pool_size=(4,4),strides=2))\n",
    "model6.add(Dropout(0.2))\n",
    "#flatten layer for full connection\n",
    "model6.add(Flatten())\n",
    "#third layer\n",
    "model6.add(Dense(1024,activation='relu'))\n",
    "#Fourth layer\n",
    "model6.add(Dense(256,activation='swish'))\n",
    "model6.add(Dropout(0.2))\n",
    "#fifth layer\n",
    "model6.add(Dense(128,activation='swish'))\n",
    "model6.add(Dropout(0.2))\n",
    "# sixth/output layer\n",
    "model6.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model6.summary()\n",
    "#compiling on the basis of sgd optimizer on same loss function\n",
    "model6.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "hist6=model6.fit(train_generator,epochs=70,validation_data=(validation_generator), batch_size=128, verbose=1)\n",
    "print(\"Model Training Complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "S-s5y0FvkRmq",
    "outputId": "08ba0356-5b1d-4d64-846f-3bbc92a0a675"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy graph\n",
    "plt.plot(hist6.history['accuracy'])\n",
    "plt.plot(hist6.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "secMMGwNj95Z",
    "outputId": "cb148076-7932-4e11-c77b-04b6bdd538a6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting loss graph\n",
    "plt.plot(hist6.history['loss'])\n",
    "plt.plot(hist6.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUxLvgM8R0wx"
   },
   "source": [
    "### Testing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrsrV-6mXRij",
    "outputId": "56779a87-411b-4461-a7e6-67c49c6492d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: Operating the radio -- Possibility: 99.99854564666748 %\n",
      "Image 2: Talking on the phone - right -- Possibility: 78.96634936332703 %\n",
      "Image 3: Talking on the phone - left -- Possibility: 99.79065656661987 %\n",
      "Image 4: Talking on the phone - right -- Possibility: 90.80380201339722 %\n",
      "Image 5: Texting - left -- Possibility: 98.76547455787659 %\n",
      "Image 6: Texting - right -- Possibility: 99.99204874038696 %\n",
      "Image 7: Texting - right -- Possibility: 86.02083921432495 %\n",
      "Image 8: Texting - left -- Possibility: 98.12450408935547 %\n",
      "Image 9: Operating the radio -- Possibility: 98.7622857093811 %\n",
      "Image 10: Operating the radio -- Possibility: 99.48786497116089 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "my_img = plt.imread(\"img1.jpg\")\n",
    "\n",
    "my_img_resized = resize(my_img,(160,160,3))\n",
    "\n",
    "probabilities = model6.predict(np.array([my_img_resized,]))\n",
    "number_to_class = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities[0,:])\n",
    "print(\"Image 1:\", number_to_class[index[9]], \"-- Possibility:\", probabilities[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 2\n",
    "my_img2 = plt.imread(\"img2.jpg\")\n",
    "my_img2_resized = resize(my_img2,(160,160,3))\n",
    "\n",
    "probabilities2 = model6.predict(np.array([my_img2_resized,]))\n",
    "number_to_class2 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities2[0,:])\n",
    "print(\"Image 2:\", number_to_class2[index[9]], \"-- Possibility:\", probabilities2[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "# image 3\n",
    "my_img3 = plt.imread(\"img3.jpg\")\n",
    "my_img3_resized = resize(my_img3,(160,160,3))\n",
    "\n",
    "probabilities3 = model6.predict(np.array([my_img3_resized,]))\n",
    "number_to_class3 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities3[0,:])\n",
    "print(\"Image 3:\", number_to_class3[index[9]], \"-- Possibility:\", probabilities3[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "# image 4\n",
    "my_img4 = plt.imread(\"img4.jpg\")\n",
    "my_img4_resized = resize(my_img4,(160,160,3))\n",
    "\n",
    "probabilities4 = model6.predict(np.array([my_img4_resized,]))\n",
    "number_to_class4 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities4[0,:])\n",
    "print(\"Image 4:\", number_to_class4[index[9]], \"-- Possibility:\", probabilities4[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 5\n",
    "my_img5 = plt.imread(\"img5.jpg\")\n",
    "my_img5_resized = resize(my_img5,(160,160,3))\n",
    "\n",
    "probabilities5 = model6.predict(np.array([my_img5_resized,]))\n",
    "number_to_class5 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities5[0,:])\n",
    "print(\"Image 5:\", number_to_class5[index[9]], \"-- Possibility:\", probabilities5[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 6\n",
    "my_img6 = plt.imread(\"img6.jpg\")\n",
    "my_img6_resized = resize(my_img6,(160,160,3))\n",
    "\n",
    "probabilities6 = model6.predict(np.array([my_img6_resized,]))\n",
    "number_to_class6 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities6[0,:])\n",
    "print(\"Image 6:\", number_to_class6[index[9]], \"-- Possibility:\", probabilities6[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 7\n",
    "my_img7 = plt.imread(\"img7.jpg\")\n",
    "my_img7_resized = resize(my_img7,(160,160,3))\n",
    "\n",
    "probabilities7 = model6.predict(np.array([my_img7_resized,]))\n",
    "number_to_class7 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities7[0,:])\n",
    "print(\"Image 7:\", number_to_class7[index[9]], \"-- Possibility:\", probabilities7[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 8\n",
    "my_img8 = plt.imread(\"img8.jpg\")\n",
    "my_img8_resized = resize(my_img8,(160,160,3))\n",
    "\n",
    "probabilities8 = model6.predict(np.array([my_img8_resized,]))\n",
    "number_to_class8 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities8[0,:])\n",
    "print(\"Image 8:\", number_to_class8[index[9]], \"-- Possibility:\", probabilities8[0,index[9]]*100, \"%\")\n",
    "\n",
    "# image 9\n",
    "my_img9 = plt.imread(\"img9.jpg\")\n",
    "my_img9_resized = resize(my_img9,(160,160,3))\n",
    "\n",
    "probabilities9 = model6.predict(np.array([my_img9_resized,]))\n",
    "number_to_class9 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities9[0,:])\n",
    "print(\"Image 9:\", number_to_class9[index[9]], \"-- Possibility:\", probabilities9[0,index[9]]*100, \"%\")\n",
    "\n",
    "# image 10\n",
    "my_img10 = plt.imread(\"img10.jpg\")\n",
    "my_img10_resized = resize(my_img10,(160,160,3))\n",
    "\n",
    "probabilities10 = model6.predict(np.array([my_img10_resized,]))\n",
    "number_to_class10 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities10[0,:])\n",
    "print(\"Image 10:\", number_to_class10[index[9]], \"-- Possibility:\", probabilities10[0,index[9]]*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeRc2YyiR69r"
   },
   "source": [
    "CNN 5 Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tu8LiVg1ZAHr",
    "outputId": "c0243e16-39e4-462e-b64c-c85ccaffeb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 64, 64, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 32, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16777344  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,861,898\n",
      "Trainable params: 16,861,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "176/176 [==============================] - 231s 1s/step - loss: 2.3460 - accuracy: 0.1081 - val_loss: 2.2964 - val_accuracy: 0.1089\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 2.2974 - accuracy: 0.1107 - val_loss: 2.2870 - val_accuracy: 0.1187\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 233s 1s/step - loss: 2.2905 - accuracy: 0.1238 - val_loss: 2.2749 - val_accuracy: 0.1323\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 234s 1s/step - loss: 2.2691 - accuracy: 0.1373 - val_loss: 2.2164 - val_accuracy: 0.1809\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 226s 1s/step - loss: 2.1949 - accuracy: 0.1852 - val_loss: 2.0920 - val_accuracy: 0.2426\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 226s 1s/step - loss: 2.0724 - accuracy: 0.2341 - val_loss: 1.9597 - val_accuracy: 0.3057\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 226s 1s/step - loss: 1.9632 - accuracy: 0.2809 - val_loss: 1.8117 - val_accuracy: 0.3307\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 227s 1s/step - loss: 1.8663 - accuracy: 0.3061 - val_loss: 1.7032 - val_accuracy: 0.3831\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 1.7971 - accuracy: 0.3303 - val_loss: 1.6392 - val_accuracy: 0.3957\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 1.7385 - accuracy: 0.3483 - val_loss: 1.5753 - val_accuracy: 0.4122\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.6883 - accuracy: 0.3647 - val_loss: 1.4942 - val_accuracy: 0.4518\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 1.6391 - accuracy: 0.3773 - val_loss: 1.4654 - val_accuracy: 0.4439\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.6138 - accuracy: 0.3888 - val_loss: 1.4334 - val_accuracy: 0.4622\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.5995 - accuracy: 0.3975 - val_loss: 1.3915 - val_accuracy: 0.4740\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 1.5665 - accuracy: 0.4072 - val_loss: 1.3447 - val_accuracy: 0.4949\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.5262 - accuracy: 0.4196 - val_loss: 1.3280 - val_accuracy: 0.5128\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.5129 - accuracy: 0.4282 - val_loss: 1.3008 - val_accuracy: 0.5171\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 231s 1s/step - loss: 1.4992 - accuracy: 0.4353 - val_loss: 1.2972 - val_accuracy: 0.5260\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.4775 - accuracy: 0.4424 - val_loss: 1.2476 - val_accuracy: 0.5379\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.4464 - accuracy: 0.4510 - val_loss: 1.2413 - val_accuracy: 0.5519\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 232s 1s/step - loss: 1.4268 - accuracy: 0.4556 - val_loss: 1.2199 - val_accuracy: 0.5733\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 236s 1s/step - loss: 1.4213 - accuracy: 0.4601 - val_loss: 1.2519 - val_accuracy: 0.5425\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 227s 1s/step - loss: 1.3993 - accuracy: 0.4709 - val_loss: 1.1835 - val_accuracy: 0.5698\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 225s 1s/step - loss: 1.3769 - accuracy: 0.4821 - val_loss: 1.1543 - val_accuracy: 0.5887\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 225s 1s/step - loss: 1.3590 - accuracy: 0.4905 - val_loss: 1.1283 - val_accuracy: 0.6019\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 223s 1s/step - loss: 1.3377 - accuracy: 0.4988 - val_loss: 1.1154 - val_accuracy: 0.6025\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 223s 1s/step - loss: 1.3262 - accuracy: 0.5023 - val_loss: 1.1330 - val_accuracy: 0.5955\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 224s 1s/step - loss: 1.3191 - accuracy: 0.5050 - val_loss: 1.0892 - val_accuracy: 0.6125\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 224s 1s/step - loss: 1.2980 - accuracy: 0.5134 - val_loss: 1.0619 - val_accuracy: 0.6360\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 226s 1s/step - loss: 1.2899 - accuracy: 0.5199 - val_loss: 1.0356 - val_accuracy: 0.6323\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 226s 1s/step - loss: 1.2666 - accuracy: 0.5301 - val_loss: 1.0260 - val_accuracy: 0.6459\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 227s 1s/step - loss: 1.2555 - accuracy: 0.5300 - val_loss: 1.0063 - val_accuracy: 0.6506\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.2389 - accuracy: 0.5375 - val_loss: 1.0109 - val_accuracy: 0.6455\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.2256 - accuracy: 0.5425 - val_loss: 1.0000 - val_accuracy: 0.6513\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.2130 - accuracy: 0.5471 - val_loss: 0.9425 - val_accuracy: 0.6688\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 229s 1s/step - loss: 1.1946 - accuracy: 0.5573 - val_loss: 0.9492 - val_accuracy: 0.6714\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.1778 - accuracy: 0.5611 - val_loss: 0.9083 - val_accuracy: 0.6855\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.1690 - accuracy: 0.5645 - val_loss: 0.8962 - val_accuracy: 0.6897\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.1666 - accuracy: 0.5686 - val_loss: 0.8903 - val_accuracy: 0.6900\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 227s 1s/step - loss: 1.1531 - accuracy: 0.5704 - val_loss: 0.8705 - val_accuracy: 0.7078\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 225s 1s/step - loss: 1.1409 - accuracy: 0.5781 - val_loss: 0.8759 - val_accuracy: 0.6999\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 228s 1s/step - loss: 1.1300 - accuracy: 0.5801 - val_loss: 0.8446 - val_accuracy: 0.7133\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 225s 1s/step - loss: 1.1083 - accuracy: 0.5881 - val_loss: 0.8575 - val_accuracy: 0.7017\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 227s 1s/step - loss: 1.1167 - accuracy: 0.5894 - val_loss: 0.8390 - val_accuracy: 0.7077\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 230s 1s/step - loss: 1.0975 - accuracy: 0.5934 - val_loss: 0.8325 - val_accuracy: 0.7270\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 224s 1s/step - loss: 1.0960 - accuracy: 0.5956 - val_loss: 0.8094 - val_accuracy: 0.7205\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 223s 1s/step - loss: 1.0787 - accuracy: 0.6052 - val_loss: 0.8153 - val_accuracy: 0.7246\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 223s 1s/step - loss: 1.0753 - accuracy: 0.6038 - val_loss: 0.8077 - val_accuracy: 0.7233\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 224s 1s/step - loss: 1.0638 - accuracy: 0.6118 - val_loss: 0.7927 - val_accuracy: 0.7217\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 225s 1s/step - loss: 1.0564 - accuracy: 0.6106 - val_loss: 0.7744 - val_accuracy: 0.7395\n",
      "Model Training Complete...\n"
     ]
    }
   ],
   "source": [
    "#CNN 5 Layer model\n",
    "#initialising the model\n",
    "model12=Sequential()\n",
    "#first input layer\n",
    "model12.add(Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(128,128,3)))\n",
    "model12.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model12.add(Dropout(0.2))\n",
    "#second layer\n",
    "model12.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model12.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model12.add(Dropout(0.2))\n",
    "#to make a full connection flatten layer is used\n",
    "model12.add(Flatten())\n",
    "#third layer\n",
    "model12.add(Dense(128,activation='relu'))\n",
    "model12.add(Dropout(0.5))\n",
    "#fourth layer\n",
    "model12.add(Dense(64,activation='relu'))\n",
    "#output layer\n",
    "model12.add(Dense(10, activation='softmax'))\n",
    "model12.summary()\n",
    "#compiling the model on the basis of categorical crossentropy\n",
    "model12.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])\n",
    "hist12=model12.fit(train_generator,epochs=50,validation_data=(validation_generator), verbose=1)\n",
    "print(\"Model Training Complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDfsL0GFAAme"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptoD18gBQpzZ"
   },
   "outputs": [],
   "source": [
    "#CNN 5 Layer Model 8X8\n",
    "model10 = Sequential() \n",
    "\n",
    "\n",
    "# first layer of the neural network. \n",
    "model10.add(Conv2D(32,(8,8), activation = 'swish', padding = 'same', input_shape = (160,160,3)))\n",
    "model10.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model10.add(Dropout(0.2))\n",
    "#second layer\n",
    "model10.add(Conv2D(128,(8,8), activation = 'swish', padding = 'same'))\n",
    "model10.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model10.add(Flatten())\n",
    "#3rd layer\n",
    "model10.add(Dense(128, activation = 'swish'))\n",
    "model10.add(Dropout(0.2))\n",
    "#4th layer\n",
    "model10.add(Dense(64, activation = 'swish'))\n",
    "model10.add(Dropout(0.2))\n",
    "#5th layer\n",
    "model10.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model10.summary()\n",
    "model10.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])\n",
    "hist10=model10.fit(train_generator,epochs=100,validation_data=validation_generator, verbose=1)\n",
    "print(\"Model Training Complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbmwaRs4Qzwh"
   },
   "outputs": [],
   "source": [
    "ImplementingVGG16=tf.keras.applications.VGG16(input_shape=(160,160,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "mod1=ImplementingVGG16.output\n",
    "mod1=tf.keras.layers.Flatten()(mod1)\n",
    "mod1=tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(mod1)\n",
    "mod1=tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(mod1)\n",
    "output1=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod1)\n",
    "model1= tf.keras.models.Model(inputs=ImplementingVGG16.inputs,outputs=output1)\n",
    "\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(0.00000005),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
    "             metrics=['accuracy'])\n",
    "model1.summary()\n",
    "hist1=model1.fit(train_generator,epochs=50,validation_data=(validation_generator), verbose=1)\n",
    "print(\"Model Training Complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYrkYBJqQ5OJ"
   },
   "outputs": [],
   "source": [
    "ImplementingResNet=tf.keras.applications.resnet.ResNet50(include_top= False, \n",
    "                                                         weights='imagenet',\n",
    "                                                         input_shape= (160,160,3))\n",
    "ImplementingResNet.summary()\n",
    "mod=ImplementingResNet.output\n",
    "mod=tf.keras.layers.Flatten()(mod)\n",
    "mod=tf.keras.layers.Dense(units=32, activation=tf.nn.relu)(mod)\n",
    "mod=tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(mod)\n",
    "output=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod)\n",
    "model8= tf.keras.models.Model(inputs=ImplementingResNet.inputs,outputs=output)\n",
    "\n",
    "model8.compile(optimizer=tf.keras.optimizers.Adam(0.0000005),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
    "             metrics=['accuracy'])\n",
    "model8.summary()\n",
    "hist8=model8.fit(X_train,Y_train,epochs=20,validation_data=(X_test,Y_test), verbose=1)\n",
    "print(\"Model Training Complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DS7XqFjBRAFR",
    "outputId": "96e67962-e24e-408a-a554-ce4fc92af859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "9420800/9406464 [==============================] - 0s 0us/step\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 64, 64, 32)   864         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 64, 64, 32)   128         ['Conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 64, 64, 32)   0           ['bn_Conv1[0][0]']               \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 64, 64, 32)  288         ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 64, 64, 32)  128         ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 64, 64, 32)  0           ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                                                           ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 64, 64, 16)  512         ['expanded_conv_depthwise_relu[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 64, 64, 16)  64          ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 64, 64, 96)   1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 64, 64, 96)  384         ['block_1_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 64, 64, 96)   0           ['block_1_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 65, 65, 96)   0           ['block_1_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 32, 32, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 32, 32, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 32, 32, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 32, 32, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 32, 32, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 32, 32, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 32, 32, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 32, 32, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 32, 32, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 32, 32, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 32, 32, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 32, 32, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 32, 32, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 32, 32, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 32, 32, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 32, 32, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 32, 32, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 33, 33, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 16, 16, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 16, 16, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 16, 16, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 16, 16, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 16, 16, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 16, 16, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 16, 16, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 16, 16, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 16, 16, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 16, 16, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 16, 16, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 16, 16, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 16, 16, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 16, 16, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 17, 17, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 8, 8, 192)   1728        ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 8, 8, 192)   768         ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 8, 8, 192)    0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 8, 8, 64)     12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 8, 8, 64)     0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 8, 8, 64)     0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 8, 8, 64)     0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 8, 8, 384)    24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 8, 8, 384)   1536        ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 8, 8, 384)    0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 8, 8, 384)   3456        ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 8, 8, 384)   1536        ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 8, 8, 384)   0           ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 8, 8, 96)     36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 8, 8, 576)   5184        ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 8, 8, 576)   2304        ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 8, 8, 576)   0           ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 8, 8, 96)     55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 8, 8, 96)     0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 8, 8, 576)   5184        ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 8, 8, 576)   2304        ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 8, 8, 576)   0           ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 8, 8, 96)     55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 8, 8, 96)     0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 9, 9, 576)    0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 4, 4, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 4, 4, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 4, 4, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 4, 4, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 4, 4, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 4, 4, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 4, 4, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 4, 4, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 4, 4, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 4, 4, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 4, 4, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 4, 4, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 4, 4, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 20480)        0           ['out_relu[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 32)           655392      ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 10)           330         ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,913,706\n",
      "Trainable params: 2,879,594\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "176/176 [==============================] - 156s 862ms/step - loss: 2.1112 - accuracy: 0.2233\n",
      "Epoch 2/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 1.0976 - accuracy: 0.6196\n",
      "Epoch 3/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 0.5534 - accuracy: 0.8171\n",
      "Epoch 4/30\n",
      "176/176 [==============================] - 152s 864ms/step - loss: 0.3486 - accuracy: 0.8870\n",
      "Epoch 5/30\n",
      "176/176 [==============================] - 152s 864ms/step - loss: 0.2710 - accuracy: 0.9102\n",
      "Epoch 6/30\n",
      "176/176 [==============================] - 152s 862ms/step - loss: 0.2174 - accuracy: 0.9309\n",
      "Epoch 7/30\n",
      "176/176 [==============================] - 152s 863ms/step - loss: 0.1835 - accuracy: 0.9421\n",
      "Epoch 8/30\n",
      "176/176 [==============================] - 152s 864ms/step - loss: 0.1593 - accuracy: 0.9478\n",
      "Epoch 9/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 0.1343 - accuracy: 0.9576\n",
      "Epoch 10/30\n",
      "176/176 [==============================] - 152s 864ms/step - loss: 0.1211 - accuracy: 0.9625\n",
      "Epoch 11/30\n",
      "176/176 [==============================] - 153s 866ms/step - loss: 0.1059 - accuracy: 0.9663\n",
      "Epoch 12/30\n",
      "176/176 [==============================] - 152s 865ms/step - loss: 0.0979 - accuracy: 0.9698\n",
      "Epoch 13/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 0.0928 - accuracy: 0.9699\n",
      "Epoch 14/30\n",
      "176/176 [==============================] - 153s 866ms/step - loss: 0.0802 - accuracy: 0.9732\n",
      "Epoch 15/30\n",
      "176/176 [==============================] - 152s 865ms/step - loss: 0.0827 - accuracy: 0.9736\n",
      "Epoch 16/30\n",
      "176/176 [==============================] - 153s 869ms/step - loss: 0.0695 - accuracy: 0.9767\n",
      "Epoch 17/30\n",
      "176/176 [==============================] - 153s 869ms/step - loss: 0.0696 - accuracy: 0.9773\n",
      "Epoch 18/30\n",
      "176/176 [==============================] - 153s 869ms/step - loss: 0.0648 - accuracy: 0.9787\n",
      "Epoch 19/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 0.0605 - accuracy: 0.9806\n",
      "Epoch 20/30\n",
      "176/176 [==============================] - 153s 868ms/step - loss: 0.0620 - accuracy: 0.9802\n",
      "Epoch 21/30\n",
      "176/176 [==============================] - 152s 865ms/step - loss: 0.0584 - accuracy: 0.9814\n",
      "Epoch 22/30\n",
      "176/176 [==============================] - 153s 870ms/step - loss: 0.0579 - accuracy: 0.9808\n",
      "Epoch 23/30\n",
      "176/176 [==============================] - 153s 866ms/step - loss: 0.0537 - accuracy: 0.9824\n",
      "Epoch 24/30\n",
      "176/176 [==============================] - 153s 865ms/step - loss: 0.0529 - accuracy: 0.9831\n",
      "Epoch 25/30\n",
      "176/176 [==============================] - 153s 867ms/step - loss: 0.0578 - accuracy: 0.9805\n",
      "Epoch 26/30\n",
      "176/176 [==============================] - 152s 865ms/step - loss: 0.0438 - accuracy: 0.9863\n",
      "Epoch 27/30\n",
      "176/176 [==============================] - 153s 868ms/step - loss: 0.0398 - accuracy: 0.9867\n",
      "Epoch 28/30\n",
      "176/176 [==============================] - 153s 866ms/step - loss: 0.0434 - accuracy: 0.9855\n",
      "Epoch 29/30\n",
      "176/176 [==============================] - 152s 866ms/step - loss: 0.0469 - accuracy: 0.9859\n",
      "Epoch 30/30\n",
      "176/176 [==============================] - 153s 866ms/step - loss: 0.0480 - accuracy: 0.9854\n",
      "Model Training Complete...\n",
      "53/53 [==============================] - 46s 862ms/step - loss: 0.1971 - accuracy: 0.9432\n",
      "accuracy: 94.32%\n"
     ]
    }
   ],
   "source": [
    "ImplementingInception=tf.keras.applications.MobileNetV2(input_shape=(160,160,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "mod2=ImplementingInception.output\n",
    "mod2=tf.keras.layers.Flatten()(mod2)\n",
    "mod2=tf.keras.layers.Dense(units=32, activation=tf.nn.relu)(mod2)\n",
    "output2=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod2)\n",
    "model2= tf.keras.models.Model(inputs=ImplementingInception.inputs,outputs=output2)\n",
    "\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
    "             metrics=['accuracy'])\n",
    "model2.summary()\n",
    "history=model2.fit(train_generator,epochs=30,verbose=1)\n",
    "print(\"Model Training Complete...\")\n",
    "(loss, accuracy) =model2.evaluate(validation_generator,batch_size=128, verbose=1)\n",
    "print(\"accuracy: {:.2f}%\".format(accuracy*100))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzBmVOJz48Nq",
    "outputId": "19e0d3d6-7145-4ef7-c748-cc57fe78bc77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "model2.save('incep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTr4kuyX4PrC"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "my_img = plt.imread(\"img1.jpg\")\n",
    "\n",
    "my_img_resized = resize(my_img,(160,160,3))\n",
    "\n",
    "probabilities = model6.predict(np.array([my_img_resized,]))\n",
    "number_to_class = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities[0,:])\n",
    "print(\"Image 1:\", number_to_class[index[9]], \"-- Possibility:\", probabilities[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 2\n",
    "my_img2 = plt.imread(\"img2.jpg\")\n",
    "my_img2_resized = resize(my_img2,(160,160,3))\n",
    "\n",
    "probabilities2 = model6.predict(np.array([my_img2_resized,]))\n",
    "number_to_class2 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities2[0,:])\n",
    "print(\"Image 2:\", number_to_class2[index[9]], \"-- Possibility:\", probabilities2[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "# image 3\n",
    "my_img3 = plt.imread(\"img3.jpg\")\n",
    "my_img3_resized = resize(my_img3,(160,160,3))\n",
    "\n",
    "probabilities3 = model6.predict(np.array([my_img3_resized,]))\n",
    "number_to_class3 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities3[0,:])\n",
    "print(\"Image 3:\", number_to_class3[index[9]], \"-- Possibility:\", probabilities3[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "\n",
    "# image 4\n",
    "my_img4 = plt.imread(\"img4.jpg\")\n",
    "my_img4_resized = resize(my_img4,(160,160,3))\n",
    "\n",
    "probabilities4 = model6.predict(np.array([my_img4_resized,]))\n",
    "number_to_class4 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities4[0,:])\n",
    "print(\"Image 4:\", number_to_class4[index[9]], \"-- Possibility:\", probabilities4[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 5\n",
    "my_img5 = plt.imread(\"img5.jpg\")\n",
    "my_img5_resized = resize(my_img5,(160,160,3))\n",
    "\n",
    "probabilities5 = model6.predict(np.array([my_img5_resized,]))\n",
    "number_to_class5 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities5[0,:])\n",
    "print(\"Image 5:\", number_to_class5[index[9]], \"-- Possibility:\", probabilities5[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 6\n",
    "my_img6 = plt.imread(\"img6.jpg\")\n",
    "my_img6_resized = resize(my_img6,(160,160,3))\n",
    "\n",
    "probabilities6 = model6.predict(np.array([my_img6_resized,]))\n",
    "number_to_class6 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities6[0,:])\n",
    "print(\"Image 6:\", number_to_class6[index[9]], \"-- Possibility:\", probabilities6[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 7\n",
    "my_img7 = plt.imread(\"img7.jpg\")\n",
    "my_img7_resized = resize(my_img7,(160,160,3))\n",
    "\n",
    "probabilities7 = model6.predict(np.array([my_img7_resized,]))\n",
    "number_to_class7 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities7[0,:])\n",
    "print(\"Image 7:\", number_to_class7[index[9]], \"-- Possibility:\", probabilities7[0,index[9]]*100, \"%\")\n",
    "\n",
    "\n",
    "# image 8\n",
    "my_img8 = plt.imread(\"img8.jpg\")\n",
    "my_img8_resized = resize(my_img8,(160,160,3))\n",
    "\n",
    "probabilities8 = model6.predict(np.array([my_img8_resized,]))\n",
    "number_to_class8 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities8[0,:])\n",
    "print(\"Image 8:\", number_to_class8[index[9]], \"-- Possibility:\", probabilities8[0,index[9]]*100, \"%\")\n",
    "\n",
    "# image 9\n",
    "my_img9 = plt.imread(\"img9.jpg\")\n",
    "my_img9_resized = resize(my_img9,(160,160,3))\n",
    "\n",
    "probabilities9 = model6.predict(np.array([my_img9_resized,]))\n",
    "number_to_class9 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities9[0,:])\n",
    "print(\"Image 9:\", number_to_class9[index[9]], \"-- Possibility:\", probabilities9[0,index[9]]*100, \"%\")\n",
    "\n",
    "# image 10\n",
    "my_img10 = plt.imread(\"img10.jpg\")\n",
    "my_img10_resized = resize(my_img10,(160,160,3))\n",
    "\n",
    "probabilities10 = model6.predict(np.array([my_img10_resized,]))\n",
    "number_to_class10 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
    "\n",
    "index = np.argsort(probabilities10[0,:])\n",
    "print(\"Image 10:\", number_to_class10[index[9]], \"-- Possibility:\", probabilities10[0,index[9]]*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhOhEGqDZHTi"
   },
   "source": [
    "#Results:\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    ">  Table 1. accuracy\n",
    "\n",
    "Model name  | Test Accuracy|Number of Epochs| Activation function| optimizers| Filters\n",
    "-------------------|------------------|-----------|-----------|-----|-----\n",
    "Resnet 50| 97.61| 10|ReLu|Adam|NA\n",
    "Vgg16|97.44| 10|ReLu|Adam|NA\n",
    "Inception v3| 94.32| 30|ReLu|Adam|NA\n",
    "CNN 6 layer| 95.9| 70|Swish|Sgd|16X16\n",
    "CNN 5 Layer| 98| 100|Swish|Sgd|8X8\n",
    "CNN 5 Layer| 73| 50|ReLu|Adam|3X3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As it is all the models have been tried with a lot of different hyperparameters that are different numbers of nodes in each layer, each model has been trained for different filter sizes, all the optimizers and activation functions have been applied and the best giving results have been presented here. This report suggests a CNN model of 6 layers as it gives 95.9 % accuracy and when given 10 random images it predicts 9 on 10 correctly with more than 95% possibility for each category. As compared to other models this model returns the best predictions on random images. Other models with better accuracy were found to overfit and give wrong prediction on single images. that can be found in other  ipynb notebooks namely: people divided and driver detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOpHOBU5Gz4E"
   },
   "source": [
    "The following images are provided in this sequence to all the models for prediction. The prediction of all 10 pictures from CNN-6 (the recommended model) has been provided below for verification purposes.  For all other models, code has been provided along with the data for comparison and verification. Due to the limited amount of time and resources, these were the best results that were acquired. All the models have been trained on Google colab pro and hence only an ipynb file can be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJGQiZL6SuYy"
   },
   "source": [
    "## Future Scope\n",
    "For better results, there can be more work done on pre-trained models as they drastically reduce the number of hyperparameters and train quite fast. As in this project, Google Colab provides GPU to work with it was easier to train with 201 million hyperparameters too. If fewer resources are available to anyone trying to replicate this model, it is recommended to use 64X64 pixel images that can provide the accuracy but feature extraction isn’t that good. So, the results might be a bit faulty. If the resources are adequate it is recommended to use 256X256 pixel images. It can be combined to an alert system to avoid accidents and make driver concentrate towards driving or launch auto pilot in case any mishap chance is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACTXS-8CTB_o"
   },
   "source": [
    "## Reflection\n",
    "This project gave me a chance to learn about patience and how it is important to work with version control. I got to learn that different perspective in slightest of things can make a huge difference. As this markdown only contains final version of models there are 15 versions that have been saved and discarded. Also introspection of each version gave me a chance to achieve the model accuracy. This whole project took 28 days in whole to be completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jC0Jpi5SrSJ"
   },
   "source": [
    "## References:\n",
    "1.\tH. T. S. Administration, \"Distracted driving \", ed, 2017, pp. https://www.nhtsa.gov/risky-driving/distracted-driving.\n",
    "2.\tF. Omerustaoglu, C. O. Sakar, and G. Kar, \"Distracted driver detection by combining in-vehicle and image data using deep learning,\" Applied Soft Computing, vol. 96, 2020, doi: 10.1016/j.asoc.2020.106657.\n",
    "3.\tF. M. Md Rifat Arefin, Oksam Chae, Jaemyun Kim∗, \"Aggregating_CNN_and_HOG_features_for_Real-Time_Distracted_Driver_Detection.pdf.\"\n",
    "4.\tS. J. Jiao, L. Y. Liu, and Q. Liu, \"A Hybrid Deep Learning Model for Recognizing Actions of Distracted Drivers,\" Sensors (Basel), vol. 21, no. 21, Nov 8 2021, doi: 10.3390/s21217424.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvKl3_KKc1DR"
   },
   "source": [
    "NOTE:\n",
    "For implementation of other models whose execution isn't present you can run this file or I am providing other ipynb notebooks which contain their output."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
